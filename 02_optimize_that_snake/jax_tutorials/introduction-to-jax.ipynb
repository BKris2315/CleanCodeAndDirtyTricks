{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d85f66",
   "metadata": {
    "papermill": {
     "duration": 0.017627,
     "end_time": "2022-08-21T11:07:15.051621",
     "exception": false,
     "start_time": "2022-08-21T11:07:15.033994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Welcome to my JAX tutorial series. This is the first part of this series. As you notice, wherever you look at, you become encountered by something about JAX in machine learning domain, and naturally you become curious about what is really that JAX thing. I am here to explain and clarify your curiosity in the most detailed way. You can find the list of my tutorials below:\n",
    "\n",
    "\n",
    "**JAX Tutorials:**\n",
    "\n",
    "* [1. Introduction to JAX](https://www.kaggle.com/code/goktugguvercin/introduction-to-jax)\n",
    "* [2. Gradients and Jacobians in JAX](https://www.kaggle.com/code/goktugguvercin/gradients-and-jacobians-in-jax)\n",
    "* [3. Automatic Differentiation in JAX](https://www.kaggle.com/code/goktugguvercin/automatic-differentiation-in-jax)\n",
    "* [4. Just-In-Time Compilation in JAX](https://www.kaggle.com/code/goktugguvercin/just-in-time-compilation-in-jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408cc05c",
   "metadata": {
    "papermill": {
     "duration": 0.015869,
     "end_time": "2022-08-21T11:07:15.084306",
     "exception": false,
     "start_time": "2022-08-21T11:07:15.068437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> \n",
    "<img align=middle src=\"https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png\" width=\"250\" height=\"250\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d21c83",
   "metadata": {
    "papermill": {
     "duration": 0.015942,
     "end_time": "2022-08-21T11:07:15.116295",
     "exception": false,
     "start_time": "2022-08-21T11:07:15.100353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this tutorial, we will get familiar with fundamentals of JAX framework designed by Google-Brain team. It will be like general introduction to it. I guess that the things that you hear, first of all, about JAX is *GPU-Supported and Differentiable Numpy*, *Just-In-Time Compilation*, *Composable Function Transformations*, and *Automatic Vectorization*. We will deepen these titles one by one in this tutorial. In addition, I will mention about critical points that you are aware of while coding with JAX. I hope you like it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d54fda",
   "metadata": {
    "papermill": {
     "duration": 0.016045,
     "end_time": "2022-08-21T11:07:15.148400",
     "exception": false,
     "start_time": "2022-08-21T11:07:15.132355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# JAX as Improved Numpy\n",
    "\n",
    "In the simplest terms, JAX is extended and upgraded version of Numpy with additional functionalities to have remarkable performance while doing research on machine learning domain. To give more specific details about JAX, it comes up with quite handy numpy module being accessed and used by $jax.numpy \\,\\, as \\,\\, jnp$. However, while JAX tries to make its own NumPy API as similar as possible to original one in order to provide nice usability environment that we experienced so far, with the help of primitive backbone [$jax.lax$](https://jax.readthedocs.io/en/latest/jax.lax.html#module-jax.lax), it aims to equip its own NumPy API with additional features. Even though all functions in original NumPy library are not implemented in JAX, most of them are ready to be used in it. To examine the available ones, you can look at the [documentation](https://jax.readthedocs.io/en/latest/jax.numpy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54c019b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T11:07:15.185245Z",
     "iopub.status.busy": "2022-08-21T11:07:15.184042Z",
     "iopub.status.idle": "2022-08-21T11:07:17.313487Z",
     "shell.execute_reply": "2022-08-21T11:07:17.312614Z",
     "shell.execute_reply.started": "2022-03-22T15:12:29.982538Z"
    },
    "papermill": {
     "duration": 2.149109,
     "end_time": "2022-08-21T11:07:17.313718",
     "exception": false,
     "start_time": "2022-08-21T11:07:15.164609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import time\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax import grad\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82dbfc",
   "metadata": {
    "papermill": {
     "duration": 0.01592,
     "end_time": "2022-08-21T11:07:17.346309",
     "exception": false,
     "start_time": "2022-08-21T11:07:17.330389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To make a small comparison between standard numpy that we have been using so far and the one introduced by Jax framework and to be familiar with JAX as upgraded NumPy, let's we look at some subtle differences and implementation details:\n",
    "\n",
    "**1. Functional Programming Principle and Pure Functions:**\n",
    "\n",
    "Jax prefers to follow the principles of functional programming, so it avoids all possible side-effects, one of which is to modify an array with in-place manner. In other words, while immutability is at the center of jax, standard numpy enables us to create mutable arrays. To cope with this issue, Jax provides completely pure update function $x.at[i].set(y)$, which actually creates the copy of data and perform data modification over it to preserve original data untouched. Since taking the transpose of an array and reshaping it tend to violate side effect rules in same aspect, Jax prefer to work with copies of original data for those operations too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0db844",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-21T11:07:17.386659Z",
     "iopub.status.busy": "2022-08-21T11:07:17.385966Z",
     "iopub.status.idle": "2022-08-21T11:07:17.924130Z",
     "shell.execute_reply": "2022-08-21T11:07:17.924874Z",
     "shell.execute_reply.started": "2022-03-22T15:12:32.186051Z"
    },
    "papermill": {
     "duration": 0.562583,
     "end_time": "2022-08-21T11:07:17.925110",
     "exception": false,
     "start_time": "2022-08-21T11:07:17.362527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError '<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n",
      "[[1 2]\n",
      " [3 5]]\n"
     ]
    }
   ],
   "source": [
    "# ++\n",
    "x1 = np.array([[1, 2], [3, 4]])  # standard numpy array\n",
    "x2 = jnp.array([[1, 2], [3, 4]])  # jax numpy array\n",
    "\n",
    "# Exception block to catch the error posed by in-place data manipulation\n",
    "try:\n",
    "    x2[1, 1] = 5\n",
    "except Exception as ex:\n",
    "    print(type(ex).__name__, ex)\n",
    "\n",
    "# As you see, the attempt to change the data with pure modification actually creates the updated copy of data\n",
    "print(x2.at[1, 1].set(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab741d7a",
   "metadata": {
    "papermill": {
     "duration": 0.016521,
     "end_time": "2022-08-21T11:07:17.960264",
     "exception": false,
     "start_time": "2022-08-21T11:07:17.943743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**2. GPU Support:**\n",
    "\n",
    "Any numpy array created by JAX can be allocated to the accelerators, which are GPU and TPU. In other words, JAX introduces the combination of numpy's flexibility with GPU support. As you know, the devices (processing units) are split into three main categories which are CPUs, GPUs, and TPUs. Each of those three device groups is actually considered as a device backend by JAX, and by passing name of these three backends to the function [$jax.devices()$](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices) we can obtain the list of all devices under that backend, which are ready to be used with JAX.\n",
    "\n",
    "Whenever a JAX array is instantiated, it is allocated in defult device, which is $jax.devices()[0]$, and this allocation is called as ***uncommited data***. All computations of uncommited data are performed on default device, and the result of those computations is also allocated on default device. However, we can transfer the data which were uncommitted on CPU to another device such as GPU in order to accelerate the operations. This can be achieved by passing one of the devices in device list to $jax.device\\_put()$ function. At that point, you can ask yourself, as you just explained, whether we have to explicitly commit data to GPU. The answer to this question is **NO**. If you do not have any GPU and TPU in your computer, your default device naturally becomes CPU; however, the ones with those accelerators can configure their default device as GPU by setting the environment variable ***JAX_PLATFORM_NAME*** to \"gpu\", which eliminates explicitly data commitment process for every array instantiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524654dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T11:07:17.998234Z",
     "iopub.status.busy": "2022-08-21T11:07:17.997482Z",
     "iopub.status.idle": "2022-08-21T11:07:24.568965Z",
     "shell.execute_reply": "2022-08-21T11:07:24.569803Z",
     "shell.execute_reply.started": "2022-03-22T15:12:32.673649Z"
    },
    "papermill": {
     "duration": 6.592868,
     "end_time": "2022-08-21T11:07:24.570045",
     "exception": false,
     "start_time": "2022-08-21T11:07:17.977177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Devices:  [CpuDevice(id=0)]\n",
      "79.3 µs ± 355 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "A = jnp.arange(100, dtype=jnp.float32)\n",
    "B = jnp.reshape(A, newshape=(10, 10))\n",
    "\n",
    "# Explicit Data Commitment\n",
    "# I have just CPU, the ones with GPU should set backend argument to \"gpu\" for acceleration\n",
    "devices = jax.devices(backend=\"cpu\")\n",
    "print(\"All Devices: \", devices)  # list of all devices under CPU backend\n",
    "B = jax.device_put(B, devices[0])\n",
    "\n",
    "# block_until_ready() due to asynchronous dispatch, we will talk about it in further parts\n",
    "%timeit jnp.dot(B, B.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b0730",
   "metadata": {
    "papermill": {
     "duration": 0.018041,
     "end_time": "2022-08-21T11:07:24.606400",
     "exception": false,
     "start_time": "2022-08-21T11:07:24.588359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**3. DeviceArray:**\n",
    "\n",
    "Now that, we briefly covered data and computation placement on device in JAX just above. Let's we get familiar with how this data (JAX array) is created and placed on those devices.\n",
    "\n",
    "A JAX array is composed of three main components:\n",
    "\n",
    "* ***aval***: \"abstract value associated to that array\"\n",
    "\n",
    "This value actually stores the meta information about the created JAX array. While it  records the shape and data type of the array, it also maintains a boolean value about whether it is weakly or strongly typed data. \n",
    "\n",
    "* ***device***: \"optional sticky device\"\n",
    "\n",
    "In previous section, we mentioned about data commitment procedure. This data field tells us whether the data is specifically committed to particular device, if it is, which one of those device it is commited.\n",
    "\n",
    "* ***device_buffer***: \"the underlying buffer owning the on-device data\"\n",
    "\n",
    "The last but the most important data field which JAX array is composed of is device_buffer. When an array is instantiated, a buffer region on a default or requested device is allocated and the data is stored in that buffer. The first crucial thing about this buffer is that it becomes completely device-oriented. In other words, this buffer is characterized with the features of the device on which it is allocated, so by calling $device\\_buffer.device()$ function, we can learn which device our data is placed on. At this point, you can think that second data member (device) would be sufficient to learn the device of the data, **why do I need to call $device()$ function of *device_buffer* ?** In fact, second data member (device component) of JAX arrays does tell us which device it is on only when it is explicitly commited to a device. In other words, the ones created as default and not transferred to other device by $device\\_put()$ will have $None$ value for $device$ member.\n",
    "\n",
    "As you see, the created JAX arrays are strongly correlated with which device they are placed on; they store important details about them. That's why dtype of JAX arrays is called as $DeviceArray$. DeviceArrays are ndarrays backed by a single device memory buffer, and if you have two DeviceArrays commited to two different devices, you are not allowed to perform any binary operations between them. To be more familiar with the structure of DeviceArray architecture, you can check Google's github repository [DeviceArray](https://github.com/google/jax/blob/8b1cb8a5366edfbffc8a9c193c0cb433c4756d93/jax/_src/device_array.py#L47)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3318a3f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T11:07:24.648763Z",
     "iopub.status.busy": "2022-08-21T11:07:24.647985Z",
     "iopub.status.idle": "2022-08-21T11:07:24.662426Z",
     "shell.execute_reply": "2022-08-21T11:07:24.661765Z",
     "shell.execute_reply.started": "2022-03-22T15:20:32.270134Z"
    },
    "papermill": {
     "duration": 0.035858,
     "end_time": "2022-08-21T11:07:24.662584",
     "exception": false,
     "start_time": "2022-08-21T11:07:24.626726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commited DeviceArray:  [[1 2]\n",
      " [4 3]]\n",
      "Which device it is placed on:  TFRT_CPU_0\n",
      "Whether it is commited, if so where it is commited: TFRT_CPU_0\n",
      "Meta information about this array:  ShapedArray(int32[2,2])\n",
      "Data type of this array:  <class 'jaxlib.xla_extension.DeviceArray'>\n",
      "\n",
      "Uncommited DeviceArray:  [[1.3  5.6  6.7 ]\n",
      " [4.12 3.34 2.2 ]]\n",
      "Which device it is placed on:  TFRT_CPU_0\n",
      "Whether it is commited, if so where it is commited:  None\n",
      "Meta information about this array:  ShapedArray(float32[2,3])\n",
      "Data type of this array:  <class 'jaxlib.xla_extension.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "device_array1 = jnp.array([[1, 2], [4, 3]])\n",
    "device_array2 = jnp.array([[1.3, 5.6, 6.7], [4.12, 3.34, 2.2]])\n",
    "\n",
    "device_array1 = jax.device_put(device_array1, jax.devices(backend=\"cpu\")[0])\n",
    "\n",
    "print(\"Commited DeviceArray: \", device_array1)\n",
    "print(\"Which device it is placed on: \", device_array1.device_buffer.device())\n",
    "print(\"Whether it is commited, if so where it is commited:\", device_array1._device)\n",
    "print(\"Meta information about this array: \", device_array1.aval)\n",
    "print(\"Data type of this array: \", type(device_array1))\n",
    "\n",
    "print(\"\\nUncommited DeviceArray: \", device_array2)\n",
    "print(\"Which device it is placed on: \", device_array2.device_buffer.device())\n",
    "print(\"Whether it is commited, if so where it is commited: \", device_array2._device)\n",
    "print(\"Meta information about this array: \", device_array2.aval)\n",
    "print(\"Data type of this array: \", type(device_array2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65a6fe",
   "metadata": {
    "papermill": {
     "duration": 0.017547,
     "end_time": "2022-08-21T11:07:24.698776",
     "exception": false,
     "start_time": "2022-08-21T11:07:24.681229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**4. Differentiable NumPy**\n",
    "\n",
    "However, considering JAX as only GPU-supported Numpy under the functional programming principles is nothing but underestimating what JAX is capable of doing; it is absolutely more than those. Additionally, JAX can automatically differentiate the functions composed of not only python's own native characteristics like if-conditions, recursions, closures and iterations, but also jax.numpy codes. To accomplish this, it utilizes $ jax.grad()$ transformer, which takes the name of target function as input argument, and generates derivative function, that is why differentiability system in JAX is called as transformation of functions and programs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdebcfc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T11:07:24.738675Z",
     "iopub.status.busy": "2022-08-21T11:07:24.737987Z",
     "iopub.status.idle": "2022-08-21T11:07:24.878007Z",
     "shell.execute_reply": "2022-08-21T11:07:24.878533Z",
     "shell.execute_reply.started": "2022-03-22T15:12:39.970771Z"
    },
    "papermill": {
     "duration": 0.161619,
     "end_time": "2022-08-21T11:07:24.878706",
     "exception": false,
     "start_time": "2022-08-21T11:07:24.717087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.75\n",
      "2.7182817\n"
     ]
    }
   ],
   "source": [
    "# native python function\n",
    "def f(x):\n",
    "    return x**3\n",
    "\n",
    "# jax-numpy function\n",
    "def g(x):\n",
    "    return jnp.exp(x)\n",
    "\n",
    "# What jax.grad() returns is a function; it generates a function from a function, so it is a transformer\n",
    "grad_f = grad(f)  # 3 * x * x\n",
    "grad_g = grad(g)  # exp(x)\n",
    "\n",
    "print(grad_f(2.5)) # 3 * 2.5 * 2.5 = 18.75\n",
    "print(grad_g(1.0)) # exp(1) = e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3488c",
   "metadata": {
    "papermill": {
     "duration": 0.018332,
     "end_time": "2022-08-21T11:07:24.915877",
     "exception": false,
     "start_time": "2022-08-21T11:07:24.897545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compilation in JAX:\n",
    "\n",
    "XLA is a domain-oriented compiler designed to accelerate all linear algebra based operations in machine learning models; however, it also enables device memory to be much more efficiently used apart from the improvements in speed. It was released as a part of TensorFlow framework, but now it is also used with JAX. In other words, JAX utilizes [XLA](https://www.tensorflow.org/xla) engine to compile and run its own NumPy codes on the accelerators. To mention about the behavior of XLA briefly, it splits and compiles the machine learning model or just a small python function abounding with a couple of operations into smaller but unique pieces, called as XLA-optimized kernels. Without XLA optimization, each kernel represents just one operation, XLA tries to fuse them as much as possible into single GPU kernel.\n",
    "\n",
    "At this point, you can think and say that this is not a new concept; it exists for a long time with Tensorflow. However, JAX, with the help of $jax.jit()$ transformer, becomes capable of doing **Just In-Time (JIT)** compilation, which is the concept of compiling the source code at run time just before executing it, to have efficient code execution in XLA. We will examine JIT structure of JAX in detail  on further notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0969ede",
   "metadata": {
    "papermill": {
     "duration": 0.018537,
     "end_time": "2022-08-21T11:07:24.952937",
     "exception": false,
     "start_time": "2022-08-21T11:07:24.934400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Asynchronous Dispatch in JAX:\n",
    "\n",
    "Calling a function in Python is a little bit expensive compared with other programming languages, since Python opts for dynamic typing, and this poses great type inspection overhead. To get into more detail, arguments, variables and return value of python functions are determined at run-time; so function calls in python can be more costly. To alleviate these overheads, asynchronous dispatch is preferred to be used in JAX.\n",
    "\n",
    "Whenever a function is invoked to perform some kinds of computations like dot product of two matrices or SVD of a matrix, it is waited to complete entire operation. JAX, with the help of [*asynchronous dispatch*](https://jax.readthedocs.io/en/latest/async_dispatch.html), gives up waiting for its entire completion, and directly returns a DeviceArray. At that point, you wonder how the result is computed and returned without waiting until the calculation is finished. In fact, returned JAX array is a future value that will be produced and allocated on a device; it is currently not available. You can think like it is an empty array but it consists of meta information about the structure of the result; hence, we are allowed to inspect the shape and type of it. Even we can use it in another computation, since the result of all computations will be dispatched asynchronously; the subsequent operations do not need exact result right away. However, if a request is created to access the exact result like printing it or converting to $numpy.ndarray$, then dispatch is synchronized, and it is waited for execution of operation to be completed. \n",
    "\n",
    "This concept is called asynchronous dispatch. To wrap up and make it a little bit more clear, if the result of the computation is not required on the host, execution of computations can be conducted asynchronously on another thread, which alleviates the excessive load on critical path of python program, that is main thread. This prevents the accelerators from being waited so much time, and thereby making it possible to enqueue the operations on the device faster than it can be execued. Hence, it is a quite useful concept. At that point, the function $block\\_until\\_ready()$, used above while mentioning gpu support in JAX, forces the program to wait for the computation to complete. In that way, it enables us to test the effect and role of asynchoronous dispatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa762f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T11:07:24.993926Z",
     "iopub.status.busy": "2022-08-21T11:07:24.993221Z",
     "iopub.status.idle": "2022-08-21T11:07:26.362155Z",
     "shell.execute_reply": "2022-08-21T11:07:26.362706Z",
     "shell.execute_reply.started": "2022-03-22T15:12:39.97255Z"
    },
    "papermill": {
     "duration": 1.391258,
     "end_time": "2022-08-21T11:07:26.362930",
     "exception": false,
     "start_time": "2022-08-21T11:07:24.971672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 s, sys: 40.9 ms, total: 1.74 s\n",
      "Wall time: 454 ms\n",
      "CPU times: user 1.7 s, sys: 30 ms, total: 1.73 s\n",
      "Wall time: 435 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[740.13153, 739.0283 , 743.0375 , ..., 740.0252 , 766.72534,\n",
       "              756.9948 ],\n",
       "             [754.3393 , 750.0729 , 747.5086 , ..., 749.75885, 771.0327 ,\n",
       "              753.9398 ],\n",
       "             [744.30945, 745.9499 , 753.9323 , ..., 752.6093 , 768.2983 ,\n",
       "              753.47156],\n",
       "             ...,\n",
       "             [749.59576, 753.1669 , 757.54675, ..., 753.0712 , 781.12213,\n",
       "              760.89844],\n",
       "             [741.66565, 727.6113 , 742.857  , ..., 735.88513, 764.0407 ,\n",
       "              747.057  ],\n",
       "             [750.45935, 753.6105 , 755.4329 , ..., 757.1083 , 784.2975 ,\n",
       "              761.195  ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PRNGKey() will be mentioned on further notebooks\n",
    "x = random.uniform(random.PRNGKey(0), (3000, 3000))\n",
    "%time jnp.dot(x, x)  # approximately 582 us\n",
    "%time jnp.dot(x, x).block_until_ready()  # approximately 22.9 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7705b",
   "metadata": {
    "papermill": {
     "duration": 0.020266,
     "end_time": "2022-08-21T11:07:26.403034",
     "exception": false,
     "start_time": "2022-08-21T11:07:26.382768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Let's we analyze and make an assesment about the code above:***\n",
    "\n",
    "When a square matrix of $ 9 \\cdot 10^6 $ elements is multiplied by itself in two different manners. It seems like only 582 microseconds were required for entire process; however, it is not possible for such a severe operation with $ 27 \\cdot 10^9 $ multiplications to be completed in such a short time. In fact, that $582$ microseconds is nothing but the time taken to dispatch the work to helper thread; it is not measuing the execution of entire matrix multiplication. At this point, we are encountered by how asynchronously dispatch works. On the other hand, in second case, the function $block\\_until\\_ready()$ enforces the program to wait until the computation is completed, which enables us to measure true cost of the operation, that is $22.9$ miliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664baa6",
   "metadata": {
    "papermill": {
     "duration": 0.019551,
     "end_time": "2022-08-21T11:07:26.442360",
     "exception": false,
     "start_time": "2022-08-21T11:07:26.422809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Asynchronously dispatch is not guaranteed:***\n",
    "\n",
    "Even if no request comes from the host or no explicit methods to avoid the dispatch asynchronously like $block\\_until\\_ready()$ are used, asnchronously dispatch may not happen. There are a couple of reasons for this:\n",
    "\n",
    "1. HLO cost analysis is performed over flop counts to determine whether the operation is cheap or not. Depending on the decision, small elementary computations on CPU specifically are executed on main thread, rather than asynchronously on another thread. Besides, any attempt to execute cheap computations asynchronously on CPU backend has the potential to hurt the performance, so it is not a preferred way.\n",
    "\n",
    "2. The number of in-flight asyn. computations that can be carried out at once is limited, and its limitation is completely device-dependent. Exceeding this limitation with some sort of computations can completely fill the operation queue and it starts to overflow on main thread. As a result, main thread is blocked until the computations will finish.\n",
    "\n",
    "Due to these reasons, you may obtain confusing and misleading timing results when you execute the code above on CPU backend. Hence, I strcitly advise the people with only CPU to try it on Google Colab to get meaningful and informative timing results.\n",
    "\n",
    "If you want to have more detailed explanation about this topic, you can visit and examine my discussion on Github Page of Google-Jax: [Asynchronous Dispatch in JAX](https://github.com/google/jax/discussions/9895?sort=new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e9c56",
   "metadata": {
    "papermill": {
     "duration": 0.019366,
     "end_time": "2022-08-21T11:07:26.481436",
     "exception": false,
     "start_time": "2022-08-21T11:07:26.462070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Automatic Vectorization in JAX:\n",
    "\n",
    "Automatic vectorization in JAX enables mathematical calculations and operations like matrix multiplication, convolution, or transformation to be applicable to more than one item, that is batching. To get into more detail, JAX, in fact, allows us to generate batched version of a function designed for a mathematical operation with the help of a transformer $jax.vmap()$, which stands for vectorizing map. For example, we want to implement a simple neural network composed of just fully-connected (FC) layers. Assume that according to the architecture that we decided on, one of the layers in the network takes a vector of $512x1$ shape as input, and reduces its size by half. To realize this operation, we can easily implement the transformation by $512x256$ matrix over $x^TW$. **What about batching ? When more than one input are passed to the network, how do we going to batch this linear affine transformation ?**\n",
    "\n",
    "We have two answers to that question:\n",
    "\n",
    "1. Iterating over the samples (vectors) in the batch\n",
    "2. Creating a system in which all samples will be multiplied by transformation (weight) matrix at the same time\n",
    "\n",
    "First one is not recommended, since it is not efficient; it increases computational time enormously. Second method is the one that we want to opt for. If all samples (vectors) in the batch are concatenated from top to bottom to form a matrix, we can easily handle it by matrix-matrix product $XW$. This is actually what automatic vectorization does for us. Instead of trying to extend the mathematical operation from one sample to entire batch, we can directly utilize this transformer. Maybe you can ask that batching is not so difficult; why do we need to have a transformer that makes it for us ? To be honest, it is not as easy as expected for every operation. For example, you can have hard times to batch the convolution; the ones who did the homeworks of Stanford CS231 lectures know how sample indices are carefully organized in batched convolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77982a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T11:07:26.529368Z",
     "iopub.status.busy": "2022-08-21T11:07:26.528665Z",
     "iopub.status.idle": "2022-08-21T11:07:27.245687Z",
     "shell.execute_reply": "2022-08-21T11:07:27.244951Z",
     "shell.execute_reply.started": "2022-03-22T15:12:39.974779Z"
    },
    "papermill": {
     "duration": 0.744343,
     "end_time": "2022-08-21T11:07:27.245841",
     "exception": false,
     "start_time": "2022-08-21T11:07:26.501498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(25600, dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random.normal(random.PRNGKey(0), (100, 512))\n",
    "W2 = random.normal(random.PRNGKey(0), (512, 256))\n",
    "\n",
    "# assume that x is of 512x1 row vector\n",
    "def dense_layer2(x):\n",
    "    return jnp.dot(x.T, W2)\n",
    "\n",
    "def manually_batched_dense_layer2(X):\n",
    "    return jnp.dot(X, W2)\n",
    "\n",
    "vmap_batched_dense_layer2 = jax.vmap(dense_layer2)\n",
    "\n",
    "# botch functions compute its own result\n",
    "# to confirm the truth of vmap batch function, let's compare all items in the result\n",
    "# (100, 512) x (512, 256) = (100, 256) --> 25600 items\n",
    "# If all values in both result are equivalent, sum of boolean values should be 25600\n",
    "result1 = manually_batched_dense_layer2(X)\n",
    "result2 = vmap_batched_dense_layer2(X)\n",
    "jnp.sum(jnp.asarray(result1 == result2, dtype=jnp.int8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc36a4",
   "metadata": {
    "papermill": {
     "duration": 0.021284,
     "end_time": "2022-08-21T11:07:27.288381",
     "exception": false,
     "start_time": "2022-08-21T11:07:27.267097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Automatic vectorization can be actually composed with Just-In-Time (JIT) compilation for acceleration. JAX enables us to combine transformers, which makes everything in JAX much more handy, useful, and practical. We will investigate them in further notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e7d64",
   "metadata": {
    "papermill": {
     "duration": 0.019998,
     "end_time": "2022-08-21T11:07:27.329451",
     "exception": false,
     "start_time": "2022-08-21T11:07:27.309453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "* https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\n",
    "* https://jax.readthedocs.io/en/latest/jax.numpy.html\n",
    "* https://jax.readthedocs.io/en/latest/faq.html\n",
    "* https://en.wikipedia.org/wiki/Just-in-time_compilation\n",
    "* https://www.tensorflow.org/xla\n",
    "* https://jax.readthedocs.io/en/latest/async_dispatch.html#async-dispatch\n",
    "* https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.6751,
   "end_time": "2022-08-21T11:07:30.610165",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-21T11:07:04.935065",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
