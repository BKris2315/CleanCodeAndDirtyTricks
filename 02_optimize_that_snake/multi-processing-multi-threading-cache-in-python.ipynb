{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-Processing and Multi-Threading\n![Multi-processing v Multi-threading](https://miro.medium.com/v2/resize:fit:720/format:webp/1*FSEGozbKSrPGlBDf4Dl0Zg.jpeg)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T05:03:40.002651Z","iopub.execute_input":"2023-09-04T05:03:40.003014Z","iopub.status.idle":"2023-09-04T05:03:40.300225Z","shell.execute_reply.started":"2023-09-04T05:03:40.002982Z","shell.execute_reply":"2023-09-04T05:03:40.298986Z"}}},{"cell_type":"code","source":"# system specifications\n! lscpu","metadata":{"execution":{"iopub.status.busy":"2023-09-05T04:47:54.058245Z","iopub.execute_input":"2023-09-05T04:47:54.058752Z","iopub.status.idle":"2023-09-05T04:47:55.116081Z","shell.execute_reply.started":"2023-09-05T04:47:54.058709Z","shell.execute_reply":"2023-09-05T04:47:55.114574Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Architecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   46 bits physical, 48 bits virtual\nCPU(s):                          4\nOn-line CPU(s) list:             0-3\nThread(s) per core:              2\nCore(s) per socket:              2\nSocket(s):                       1\nNUMA node(s):                    1\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           79\nModel name:                      Intel(R) Xeon(R) CPU @ 2.20GHz\nStepping:                        0\nCPU MHz:                         2200.160\nBogoMIPS:                        4400.32\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       64 KiB\nL1i cache:                       64 KiB\nL2 cache:                        512 KiB\nL3 cache:                        55 MiB\nNUMA node0 CPU(s):               0-3\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Mitigation; PTE Inversion\nVulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state u\n                                 nknown\nVulnerability Meltdown:          Mitigation; PTI\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no mic\n                                 rocode; SMT Host state unknown\nVulnerability Retbleed:          Mitigation; IBRS\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled v\n                                 ia prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user\n                                  pointer sanitization\nVulnerability Spectre v2:        Mitigation; IBRS, IBPB conditional, STIBP condi\n                                 tional, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT Host state u\n                                 nknown\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtr\n                                 r pge mca cmov pat pse36 clflush mmx fxsr sse s\n                                 se2 ss ht syscall nx pdpe1gb rdtscp lm constant\n                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid \n                                 tsc_known_freq pni pclmulqdq ssse3 fma cx16 pci\n                                 d sse4_1 sse4_2 x2apic movbe popcnt aes xsave a\n                                 vx f16c rdrand hypervisor lahf_lm abm 3dnowpref\n                                 etch invpcid_single pti ssbd ibrs ibpb stibp fs\n                                 gsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms \n                                 invpcid rtm rdseed adx smap xsaveopt arat md_cl\n                                 ear arch_capabilities\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- 4 CPUs\n- 2 Core on each CPU\n- 2 Threads on each Core","metadata":{}},{"cell_type":"markdown","source":"## Task: Compute Mean on 1 Billion records\n\nComputation of mean: \n\nSay we have an array `arr` containing 100 numbers\n\n1. Standard way to compute mean <br>\n`mean = sum(arr) / 100`\n\n2. Distributing the computation <br>\n`m1 = sum(arr[0:51]) / 50` <br>\n`m2 = sum(arr[51:]) / 50` <br>\n`mean = (m1 + m2) / 2`\n\nNotice in 2nd way we are still computing the mean of entire array, but we divided the computation in 2 jobs.\n\nThe distribution method employed in this case is known as ‘Trivially Parallelize’, which refers to the ease with which the task was distributed among multiple subtasks. However, it is important to note that not all tasks are as straightforward to parallelize, and some may not be parallelizable at all.","metadata":{}},{"cell_type":"code","source":"# Sample Data\nimport numpy as np\nimport time\nn = 1000000000 # 1 billion\nnp.random.seed(41)\narr = np.random.randint(0,10,n)\nprint(len(arr))","metadata":{"execution":{"iopub.status.busy":"2023-09-05T04:47:58.618254Z","iopub.execute_input":"2023-09-05T04:47:58.618942Z","iopub.status.idle":"2023-09-05T04:48:17.865179Z","shell.execute_reply.started":"2023-09-05T04:47:58.618869Z","shell.execute_reply":"2023-09-05T04:48:17.863972Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"1000000000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Simple Execution (Single core, no threading)","metadata":{}},{"cell_type":"code","source":"def mean(a):\n    n = len(a)\n    sum = 0\n    for elem in a:\n        sum += elem\n\n    mean_ = sum/n\n    return mean_\n\nstart_time = time.time() # starttime\n\ndata_mean = mean(arr)\nprint(data_mean)\n\nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime","metadata":{"execution":{"iopub.status.busy":"2023-09-05T04:48:17.867223Z","iopub.execute_input":"2023-09-05T04:48:17.867592Z","iopub.status.idle":"2023-09-05T04:49:55.412078Z","shell.execute_reply.started":"2023-09-05T04:48:17.867556Z","shell.execute_reply":"2023-09-05T04:49:55.410822Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"4.499927063\nexecution time: 97.54 sec\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Multi-Processing\n\n### Multi-processing is a way to distribute your work/computation among multiple cores of your CPU, resulting in simultaneuos execution of multiple jobs.\n\nMultiprocessing and parallelization are related concepts, but they are not the same thing. Multiprocessing is a technique that allows a program to use multiple processes to perform tasks concurrently.\n\nOn the other hand, parallelization refers to the ability of a program to perform multiple tasks simultaneously.\n\nIn other words, multiprocessing is a specific implementation of parallelization. A program can be parallelized without using multiprocessing, but multiprocessing is one way to achieve parallelism.","metadata":{}},{"cell_type":"code","source":"!pip install multiprocess -q","metadata":{"execution":{"iopub.status.busy":"2023-09-05T04:50:16.478142Z","iopub.execute_input":"2023-09-05T04:50:16.478565Z","iopub.status.idle":"2023-09-05T04:50:30.947106Z","shell.execute_reply.started":"2023-09-05T04:50:16.478526Z","shell.execute_reply":"2023-09-05T04:50:30.945324Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from multiprocess import Process, Queue\n\ndef mean_mp(a, s, e, q):\n    n = e+1\n    sum = 0\n    for elem in a[s:n]:\n        sum += elem\n\n    mean_ = sum/(n-s)\n    q.put(mean_)\n\nn_processes = 2 # number of cores you want to utilize on the cpu\n\n'''\n    The Queue object is a special kind of object that is process-safe and can be used to store data that needs to be globally available to all cores.\n    Each core on your CPU has its own registers, and the data stored in these registers cannot be shared by other cores. \n    Therefore, the object must be stored in RAM, which is a shared memory accessible by all cores.\n'''\nq = Queue()\n\nn1 = n//n_processes\n\np1 = Process(target=mean_mp, args=(arr,0,n1,q))\np2 = Process(target=mean_mp, args=(arr,n1+1,n-1,q))\n\nstart_time = time.time() # starttime\n\np1.start()\np2.start()\n\n# for parent process\np1.join() # wait for p1 to complete\np2.join() # wait for p2 to complete\n\n# extract the means from both processes stored in the Queue and compute mean\nmean_s = []\nwhile not q.empty():\n    mean_s.append(q.get())\n\ndata_mean = np.sum(mean_s) / n_processes\nprint(data_mean)\n\nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime","metadata":{"execution":{"iopub.status.busy":"2023-09-05T04:50:33.419750Z","iopub.execute_input":"2023-09-05T04:50:33.421263Z","iopub.status.idle":"2023-09-05T04:51:36.090024Z","shell.execute_reply.started":"2023-09-05T04:50:33.421190Z","shell.execute_reply":"2023-09-05T04:51:36.088217Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"4.499927062999671\nexecution time: 62.6 sec\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It took half the time it required with 1 core which is ideal, becuase as we doubled the cores, the execution time is havled. Though it will not be always so visible but it will definitely reduce the time if the cores are available.","metadata":{}},{"cell_type":"markdown","source":"## Multi-Processing using Joblib","metadata":{}},{"cell_type":"code","source":"from joblib import Parallel, delayed\n\nn_processes = 2\nn1 = n//n_processes\n\nstart_time = time.time() # starttime\n\n#only minimal changes in the code, by using joblib. Notice we are using original mean function\nmean_s = Parallel(n_jobs=n_processes)(delayed(mean)(arr[i:i+n1]) for i in range(0,n,n1))\n\ndata_mean = np.sum(mean_s) / n_processes\nprint(data_mean)\n\nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime","metadata":{"execution":{"iopub.status.busy":"2023-09-05T04:51:36.092914Z","iopub.execute_input":"2023-09-05T04:51:36.093971Z","iopub.status.idle":"2023-09-05T04:56:56.925197Z","shell.execute_reply.started":"2023-09-05T04:51:36.093892Z","shell.execute_reply":"2023-09-05T04:56:56.923728Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"4.499927063\nexecution time: 320.78 sec\n","output_type":"stream"}]},{"cell_type":"markdown","source":"One possible reason why our 2nd code - which ideally should be doing the same thing as the above code written using multiprocessing module - takes more time than the 1st one is that joblib has some overhead in creating and managing the worker processes, as well as serializing and deserializing the arguments and results. This overhead can be significant if the tasks are relatively small or simple, as in your case. Another possible reason is that joblib does not use shared memory or memory mapping by default, but copies the data to each worker process. This can increase the memory usage and the communication cost between processes.\n\nParallelism Model: joblib uses a different parallelism model compared to multiprocess. joblib uses a \"fork\" approach, where the Python interpreter is cloned along with the parent process. This can have different performance characteristics compared to the \"spawn\" approach used by multiprocess.\n\nhttps://stackoverflow.com/questions/57706763/why-does-joblib-parallel-take-much-more-time-than-a-non-paralleled-computation","metadata":{}},{"cell_type":"markdown","source":"# Multi-Threading\n\n### Multi-threading is a way to perform your task concurrently among multiple threads of your CPU's core, resulting in concurrent execution of your code.\n\nConcurrency and multithreading are related concepts, but they are not the same thing. Concurrency refers to the ability of a program to perform multiple tasks simultaneously, while multithreading is a technique that allows a program to use multiple threads of execution to perform tasks concurrently.\n\nIn other words, concurrency is a broader concept that encompasses multithreading. A program can be concurrent without being multithreaded, but a multithreaded program is always concurrent.","metadata":{}},{"cell_type":"code","source":"from threading import Thread\n\ndef mean_mt(a, s, e, n_thread, mean_s):\n    n = e+1\n    sum = 0\n    for elem in a[s:n]:\n        sum += elem\n\n    mean_ = sum/(n-s)\n    mean_s[n_thread] = mean_\n\nn_threads = 2 # number of threads you want to utilize on a core\n\n'''\n    Queues can be used in this scenario as well, as they provide a means of storing data that is accessible to all threads. \n    Queues are stored in RAM and are accessible to all threads running on the CPU's core. \n    Since all threads running on the same core they anyway share the same memory, hence any object can be used, regardless of whether \n    it is stored in RAM or in a register. \n    There are also thread-safe objects that can be used specifically for threads, but that can be avoided for now.\n'''\nmean_s = [0]*n_threads\n\nn1 = n//n_threads\n\nt1 = Thread(target=mean_mt, args=(arr,0,n1,0,mean_s))\nt2 = Thread(target=mean_mt, args=(arr,n1+1,n-1,1,mean_s))\n\nstart_time = time.time() # starttime\n\nt1.start()\nt2.start()\n\n# for parent process\nt1.join() # wait for t1 to complete\nt2.join() # wait for t2 to complete\n\n#compute mean\ndata_mean = np.sum(mean_s) / n_threads\nprint(data_mean)\n\nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime","metadata":{"execution":{"iopub.status.busy":"2023-09-05T04:56:56.928689Z","iopub.execute_input":"2023-09-05T04:56:56.929598Z","iopub.status.idle":"2023-09-05T04:58:36.920057Z","shell.execute_reply.started":"2023-09-05T04:56:56.929538Z","shell.execute_reply":"2023-09-05T04:58:36.918456Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"4.499927062999671\nexecution time: 99.98 sec\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can do multi-threading in this machine, then why the computation is not faster? <br>\n\nBecuase the code is not suited for multi-threading, because to really multi-thread the code should have some i/o bound i.e., their should be some time where cpu is idle to run another code in the mean time when code 1 is performing i/o task. (Multi-Threading is similar to Concurrency). <br>\n\nHere the entire code is cpu bound, hence the cpu will never be idle to go back and forth to perform another tasks.\n\ni/o bound means that time when your system has to make some request and your cpu is idle waiting for the request to complete.\n\nTo make use of multi-threading your code should not be entirely CPU Bound.","metadata":{}},{"cell_type":"code","source":"'''\n    Let's take an example where we have some i/o bound operations\n'''\ndef func(a):\n    time.sleep(5) #this line is NOT cpu bound, meaning the cpu will be idle and has to wait for this to complete. We can take advantage of this using threading\n    return\n\nn_threads = 2\nn1 = n//n_threads\n\nprint(\"w/o threading\")\nstart_time = time.time() # starttime\n\nfor i in range(2):\n    func(i)\n    \nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime\n\nprint(\"\\nw/ threading\")\nstart_time = time.time() # starttime\n\nParallel(n_jobs=n_threads, prefer='threads')(delayed(func)(i) for i in range(2))\n\nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime","metadata":{"execution":{"iopub.status.busy":"2023-09-05T04:58:36.922620Z","iopub.execute_input":"2023-09-05T04:58:36.923231Z","iopub.status.idle":"2023-09-05T04:58:51.961516Z","shell.execute_reply.started":"2023-09-05T04:58:36.923172Z","shell.execute_reply":"2023-09-05T04:58:51.960009Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"w/o threading\nexecution time: 10.01 sec\n\nw/ threading\nexecution time: 5.02 sec\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice without threading the time took to complete the execution was 10 sec, but with threading, the cpu took advantage of that 5 sec to wait for code 1 to complete its i/o bound execution, and ran code 2 in the mean time, resulting in cutting the execution time by a whopping 50%.\n\nWith the cost of 1 iteration, it completed 2 iterations.","metadata":{}},{"cell_type":"markdown","source":"## Multi-Threading using Joblib","metadata":{}},{"cell_type":"code","source":"n_threads = 2\nn1 = n//n_threads\n\nstart_time = time.time() # starttime\n\n#only minimal changes in the code, by using joblib. Notice we are using original mean function\nmean_s = Parallel(n_jobs=n_threads, prefer='threads')(delayed(mean)(arr[i:i+n1+1]) for i in range(0,n,n1+1))\n\ndata_mean = np.sum(mean_s) / n_threads\nprint(data_mean)\n\nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime","metadata":{"execution":{"iopub.status.busy":"2023-09-05T04:58:51.966339Z","iopub.execute_input":"2023-09-05T04:58:51.966731Z","iopub.status.idle":"2023-09-05T05:00:32.669020Z","shell.execute_reply.started":"2023-09-05T04:58:51.966697Z","shell.execute_reply":"2023-09-05T05:00:32.667729Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"4.499927062999671\nexecution time: 100.7 sec\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Caching using Joblib\n\nThe function caching store the input-output of the function as a key-value pair in memory, so to not always run the function for the same input passed before.","metadata":{}},{"cell_type":"code","source":"# !rm -r joblib","metadata":{"execution":{"iopub.status.busy":"2023-09-05T05:00:32.670365Z","iopub.execute_input":"2023-09-05T05:00:32.670808Z","iopub.status.idle":"2023-09-05T05:00:32.676399Z","shell.execute_reply.started":"2023-09-05T05:00:32.670772Z","shell.execute_reply":"2023-09-05T05:00:32.674999Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from joblib import Memory\ncachedir = \"./\"\nmem = Memory(cachedir)\n\ndef func(a):\n    time.sleep(5) # deliberate; consider this to be some computation\n    return a+1\n\nfunc_c = mem.cache(func)\n\nprint(\"w/o Cache\")\nstart_time = time.time() #startime\nprint(func_c(10)) # require time\nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime\n\nprint()\nprint(\"w/ Cache\")\n\nstart_time = time.time() #startime\nprint(func_c(10)) # NO time required - present in cache\nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime\n\nprint()\n\nstart_time = time.time() #startime\nprint(func_c(20)) # require time\nprint(f\"execution time: {round(time.time() - start_time, 2)} sec\") # endtime","metadata":{"execution":{"iopub.status.busy":"2023-09-05T05:00:32.678358Z","iopub.execute_input":"2023-09-05T05:00:32.678858Z","iopub.status.idle":"2023-09-05T05:00:42.710403Z","shell.execute_reply.started":"2023-09-05T05:00:32.678813Z","shell.execute_reply":"2023-09-05T05:00:42.708875Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"w/o Cache\n________________________________________________________________________________\n[Memory] Calling __main__--tmp-ipykernel-631066686.func...\nfunc(10)\n_____________________________________________________________func - 5.0s, 0.1min\n11\nexecution time: 5.01 sec\n\nw/ Cache\n11\nexecution time: 0.0 sec\n\n________________________________________________________________________________\n[Memory] Calling __main__--tmp-ipykernel-631066686.func...\nfunc(20)\n_____________________________________________________________func - 5.0s, 0.1min\n21\nexecution time: 5.01 sec\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The 1st time it took about 5 secs, but as the function is cached and for the input a=10 the cache was available, the time required for 2nd call is just retrieving the data from the cache.\n\nIf we will pass another input which is not present in the cache to be mapped, then it will have to execute the entire funciton and store the input-output as a key-value pair in the cache.","metadata":{}},{"cell_type":"markdown","source":"# End\n\n![](https://media.giphy.com/media/mPKa6OI5oRsmextwBq/giphy.gif)","metadata":{}}]}